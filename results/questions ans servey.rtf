{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 VQA approaches:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 1. Joint embedding: ex. MCB\
2. Attention mechanisms\
3. Neural Module networks\
4. Dynamic Memory networks\
5. Using external database\
6. Using dialog (new)\
\
-> grounding of image, found 1 (visual genome, but need scene graph), and attention mechanism (but didn\'92t work well) problem: exists ground truth, but actually use it?\
-> reasoning between images -> found NMN, TransE\
\
Also how to model questions: LSTM or parsing\
\
How human do VQA\
1. For only things: we focus on the region that is relevant, and use the original knowledge. (ex.: what color)  (top-down + bottom-up(faster R-cnn) )\
2. For reasoning / counting: we also focus on the region that is relevant, but there should be multiple region. Then we use our skill for thinking.  (Neural Module network, VTransE)\
Current net, it is possible that even attend to the correct region, still get wrong. (Reasoning)}